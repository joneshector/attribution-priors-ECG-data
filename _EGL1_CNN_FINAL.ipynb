{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d355be39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading  ./train_patients.csv\n",
      "Loading  ./test_patients.csv\n",
      "(206312, 277)\n",
      "(14380, 277)\n",
      "(206312, 275)\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "epoch:  1\n",
      "epoch:  2\n",
      "epoch:  3\n",
      "epoch:  4\n",
      "epoch:  5\n",
      "epoch:  6\n",
      "epoch:  7\n",
      "epoch:  8\n",
      "epoch:  9\n",
      "epoch:  10\n",
      "Fold test accuracy: 0.9896\n",
      "trial accuracy:  0.9896384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TheAn\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:4526: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: loss of 0.07023150473833084; accuracy of 98.96383881568909%\n",
      "Time taken for repeat:  5952.224355220795\n",
      "Time taken:  5963.820904016495\n"
     ]
    }
   ],
   "source": [
    "# Imports:\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import *\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "length = 277\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "from os import path\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "from keras.models import Sequential,Input,Model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv1D, Conv2D, MaxPooling2D, MaxPooling1D\n",
    "#from keras.layers.normalization import BatchNormalization\n",
    "from tensorflow.python.keras.layers import BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "import eager_ops_fixed_ig\n",
    "\n",
    "from keras.constraints import maxnorm\n",
    "\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "\n",
    "\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import time\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "# Load the training and testing data:\n",
    "train_values = np.empty(shape=[0, length])\n",
    "test_values = np.empty(shape=[0, length])\n",
    "\n",
    "train_beats = glob.glob('./train_patients.csv')\n",
    "test_beats = glob.glob('./test_patients.csv')\n",
    "\n",
    "for j in train_beats:\n",
    "    print('Loading ', j)\n",
    "    csvrows = np.loadtxt(j, delimiter=',')\n",
    "    train_values = np.append(train_values, csvrows, axis=0)\n",
    "\n",
    "for j in test_beats:\n",
    "    print('Loading ', j)\n",
    "    csvrows = np.loadtxt(j, delimiter=',')\n",
    "    test_values = np.append(test_values, csvrows, axis=0)\n",
    "    \n",
    "print(train_values.shape)\n",
    "print(test_values.shape)\n",
    "\n",
    "# Separate the training and testing data, and one-hot encode Y:\n",
    "X_train = train_values[:,:-2]\n",
    "X_test = test_values[:,:-2]\n",
    "y_train = train_values[:,-2]\n",
    "y_test = test_values[:,-2]\n",
    "\n",
    "print(X_train.shape)\n",
    "\n",
    "## created by Fani\n",
    "def showResults(test, pred, model_name):\n",
    "    accuracy = accuracy_score(test, pred)\n",
    "    precision= precision_score(test, pred, average='macro')\n",
    "    recall = recall_score(test, pred, average = 'macro')\n",
    "    f1score_macro = f1_score(test, pred, average='macro') \n",
    "    f1score_micro = f1_score(test, pred, average='micro') \n",
    "    print(\"Accuracy  : {}\".format(accuracy))\n",
    "    print(\"Precision : {}\".format(precision))\n",
    "    print(\"Recall : {}\".format(recall))\n",
    "    print(\"f1score macro : {}\".format(f1score_macro))\n",
    "    print(\"f1score micro : {}\".format(f1score_micro))\n",
    "    cm=confusion_matrix(test, pred, labels=[1,2,3,4,5,6,7,8])\n",
    "    return (model_name, round(accuracy,3), round(precision,3) , round(recall,3) , round(f1score_macro,3), \n",
    "            round(f1score_micro, 3), cm)\n",
    "\n",
    "#### TRAINING STEP\n",
    "@tf.function\n",
    "def train_step(inputs, labels, model, lamb):\n",
    "    '''\n",
    "    Takes a single step of training. \n",
    "\n",
    "    Args:\n",
    "        inputs: A tensor. A batch of input to the model.\n",
    "        labels: A tensor. The labels to use when training.\n",
    "        model: A tf.keras.Model object, or subclass thereof.\n",
    "    Returns:\n",
    "        The predictions of the model on the inputs. Useful if you need to update metrics after training.\n",
    "    '''\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(inputs)\n",
    "        predictions = model(inputs, training=True)\n",
    "    \n",
    "        pred_loss = model.compiled_loss(labels, predictions)\n",
    "        total_loss = pred_loss\n",
    "\n",
    "        if len(model.losses) > 0:\n",
    "            regularization_loss = tf.math.add_n(model.losses)\n",
    "            total_loss = total_loss + regularization_loss\n",
    "\n",
    "        attributions        = eager_ops_fixed_ig.expected_gradients(inputs, labels, model)\n",
    "\n",
    "        standardised_attributions = (attributions - tf.math.reduce_mean(attributions)) / tf.math.reduce_std(attributions)\n",
    "\n",
    "        attr_adj_differences = tf.experimental.numpy.diff(standardised_attributions, axis=1)\n",
    "\n",
    "        total_variation_attributions = tf.reduce_sum(tf.math.abs(attr_adj_differences))\n",
    "\n",
    "        attribution_loss = lamb * total_variation_attributions\n",
    "        total_loss          = total_loss + attribution_loss\n",
    "\n",
    "    gradients = tape.gradient(total_loss, model.trainable_variables)\n",
    "    model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    #model.compiled_metrics.update_state(labels, predictions)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "overall_start_time = time.time()\n",
    "\n",
    "num_epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "X_train1 = X_train.reshape(-1, X_train.shape[1], 1)\n",
    "X_test1 = X_test.reshape(-1, X_train.shape[1], 1)\n",
    "\n",
    "X_train1 = X_train1.astype('float32')\n",
    "X_test1 = X_test1.astype('float32')\n",
    "y_train1 = to_categorical(y_train)\n",
    "y_test1 = to_categorical(y_test)\n",
    "\n",
    "\n",
    "overall_start_time = time.time()\n",
    "\n",
    "train_acc_fn = tf.keras.metrics.CategoricalAccuracy()\n",
    "test_acc_fn  = tf.keras.metrics.CategoricalAccuracy()\n",
    "\n",
    "#lamb = 0.00011\n",
    "lamb = 0.000007\n",
    "repeat_num=1\n",
    "fold_no = 1\n",
    "\n",
    "repeat_start_time = time.time()\n",
    "\n",
    "  # Define the model architecture\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv1D(filters=128, kernel_size=16,padding='same', activation='relu',input_shape=(275,1), kernel_constraint=maxnorm(5)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv1D(filters=32, kernel_size=16,padding='same', activation='relu', kernel_constraint=maxnorm(5)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv1D(filters=9, kernel_size=16,padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=4,padding='same'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu', kernel_constraint=maxnorm(5)))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(9, activation='softmax'))\n",
    "model.compile(optimizer='Nadam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "# Generate a print\n",
    "print('------------------------------------------------------------------------')\n",
    "print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "\n",
    "# Prepare the training dataset.\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train1, y_train1))\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "\n",
    "# train model over epochs\n",
    "for epoch in range(num_epochs):\n",
    "    # Fit data to model\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        predictions = train_step(x_batch_train, y_batch_train, model, lamb)\n",
    "\n",
    "    print('epoch: ', epoch+1)\n",
    "\n",
    "pred_logits = []\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test1, y_test1))\n",
    "test_dataset = test_dataset.batch(batch_size)\n",
    "for step, (x_batch_test, y_batch_test) in enumerate(test_dataset):\n",
    "    test_logits = model(x_batch_test)\n",
    "    pred_logits.append(test_logits)\n",
    "    test_acc_fn(y_batch_test, test_logits)\n",
    "test_acc = test_acc_fn.result().numpy()\n",
    "print('Fold test accuracy: {:.4f}'.format(test_acc))\n",
    "\n",
    "pred = np.concatenate(pred_logits)\n",
    "\n",
    "fold_pred_fname = './eg_gradients_l1_cnn_v6_P_pred_' + str(repeat_num) + '_' + str(fold_no) + '.csv'\n",
    "fold_test_fname = './eg_gradients_l1_cnn_v6_P_test_' + str(repeat_num) + '_' + str(fold_no) + '.csv'\n",
    "\n",
    "\n",
    "with open(fold_pred_fname, \"wb\") as fin:\n",
    "    np.savetxt(fin, pred, delimiter=\",\",fmt='%s')\n",
    "\n",
    "with open(fold_test_fname, \"wb\") as fin:\n",
    "    np.savetxt(fin, y_test1, delimiter=\",\",fmt='%s')\n",
    "\n",
    "print('trial accuracy: ', train_acc_fn(y_test1, pred).numpy())\n",
    "scores = model.evaluate(X_test1, y_test1, verbose=0)\n",
    "\n",
    "print(f'Score: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "\n",
    "\n",
    "print('Time taken for repeat: ', time.time() - repeat_start_time)\n",
    "\n",
    "\n",
    "print('Time taken: ', time.time() - overall_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50162e11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
